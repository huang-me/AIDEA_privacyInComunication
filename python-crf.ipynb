{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIDEA WITH PYTHON-CRFSUITE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data name\n",
    "# input_name = 'data/SampleData_deid.txt'\n",
    "# input_name = 'data/train_1_update.txt'\n",
    "input_name = 'data/new.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load train file\n",
    "def load_input(name):\n",
    "    # list to store train_data [contents]\n",
    "    train_set = list()\n",
    "    # list to store position \n",
    "    # [article_id, start_pos, end_pos, entity_text, entity_type]\n",
    "    position = list()\n",
    "    # dict to store mentions ['word', 'type']\n",
    "    mentions = dict()\n",
    "    # open input file\n",
    "    with open(name, 'r', encoding='utf8') as file:\n",
    "        file_text = file.read().encode('utf-8').decode('utf-8-sig')\n",
    "    # split into datas\n",
    "    datas = file_text.split('\\n\\n--------------------\\n\\n')[:-1]\n",
    "    # extract data from content\n",
    "    for data in datas:\n",
    "        data = data.split('\\n')\n",
    "        content = data[0]\n",
    "        train_set.append(content)\n",
    "        pos = data[1:]\n",
    "        for p in pos[1:]:\n",
    "            p = p.split('\\t')\n",
    "            position.extend(p)\n",
    "            mentions[p[3]] = p[4]\n",
    "    return train_set, position, mentions\n",
    "\n",
    "train_set, position, mentions = load_input(input_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CRFFormatData(trainingset, position, path):\n",
    "    if (os.path.isfile(path)):\n",
    "        os.remove(path)\n",
    "    outputfile = open(path, 'a', encoding= 'utf-8')\n",
    "\n",
    "    # output file lines\n",
    "    count = 0 # annotation counts in each content\n",
    "    tagged = list()\n",
    "    for article_id in range(len(trainingset)):\n",
    "        trainingset_split = list(trainingset[article_id])\n",
    "        while '' or ' ' in trainingset_split:\n",
    "            if '' in trainingset_split:\n",
    "                trainingset_split.remove('')\n",
    "            else:\n",
    "                trainingset_split.remove(' ')\n",
    "        start_tmp = 0\n",
    "        for position_idx in range(0,len(position),5):\n",
    "            if int(position[position_idx]) == article_id:\n",
    "                count += 1\n",
    "                if count == 1:\n",
    "                    start_pos = int(position[position_idx+1])\n",
    "                    end_pos = int(position[position_idx+2])\n",
    "                    entity_type=position[position_idx+4]\n",
    "                    if start_pos == 0:\n",
    "                        token = list(trainingset[article_id][start_pos:end_pos])\n",
    "                        whole_token = trainingset[article_id][start_pos:end_pos]\n",
    "                        for token_idx in range(len(token)):\n",
    "                            if len(token[token_idx].replace(' ','')) == 0:\n",
    "                                continue\n",
    "                            # BIO states\n",
    "                            if token_idx == 0:\n",
    "                                label = 'B-'+entity_type\n",
    "                            else:\n",
    "                                label = 'I-'+entity_type\n",
    "                            \n",
    "                            output_str = token[token_idx] + ' ' + label + '\\n'\n",
    "                            outputfile.write(output_str)\n",
    "\n",
    "                    else:\n",
    "                        token = list(trainingset[article_id][0:start_pos])\n",
    "                        whole_token = trainingset[article_id][0:start_pos]\n",
    "                        for token_idx in range(len(token)):\n",
    "                            if len(token[token_idx].replace(' ','')) == 0:\n",
    "                                continue\n",
    "                            \n",
    "                            output_str = token[token_idx] + ' ' + 'O' + '\\n'\n",
    "                            outputfile.write(output_str)\n",
    "\n",
    "                        token = list(trainingset[article_id][start_pos:end_pos])\n",
    "                        whole_token = trainingset[article_id][start_pos:end_pos]\n",
    "                        for token_idx in range(len(token)):\n",
    "                            if len(token[token_idx].replace(' ','')) == 0:\n",
    "                                continue\n",
    "                            # BIO states\n",
    "                            if token[0] == '':\n",
    "                                if token_idx == 1:\n",
    "                                    label = 'B-'+entity_type\n",
    "                                else:\n",
    "                                    label = 'I-'+entity_type\n",
    "                            else:\n",
    "                                if token_idx == 0:\n",
    "                                    label = 'B-'+entity_type\n",
    "                                else:\n",
    "                                    label = 'I-'+entity_type\n",
    "\n",
    "                            output_str = token[token_idx] + ' ' + label + '\\n'\n",
    "                            outputfile.write(output_str)\n",
    "\n",
    "                    start_tmp = end_pos\n",
    "                else:\n",
    "                    start_pos = int(position[position_idx+1])\n",
    "                    end_pos = int(position[position_idx+2])\n",
    "                    entity_type=position[position_idx+4]\n",
    "                    if start_pos<start_tmp:\n",
    "                        continue\n",
    "                    else:\n",
    "                        token = list(trainingset[article_id][start_tmp:start_pos])\n",
    "                        whole_token = trainingset[article_id][start_tmp:start_pos]\n",
    "                        for token_idx in range(len(token)):\n",
    "                            if len(token[token_idx].replace(' ','')) == 0:\n",
    "                                continue\n",
    "                            output_str = token[token_idx] + ' ' + 'O' + '\\n'\n",
    "                            outputfile.write(output_str)\n",
    "\n",
    "                    token = list(trainingset[article_id][start_pos:end_pos])\n",
    "                    whole_token = trainingset[article_id][start_pos:end_pos]\n",
    "                    for token_idx in range(len(token)):\n",
    "                        if len(token[token_idx].replace(' ','')) == 0:\n",
    "                            continue\n",
    "                        # BIO states\n",
    "                        if token[0] == '':\n",
    "                            if token_idx == 1:\n",
    "                                label = 'B-'+entity_type\n",
    "                            else:\n",
    "                                label = 'I-'+entity_type\n",
    "                        else:\n",
    "                            if token_idx == 0:\n",
    "                                label = 'B-'+entity_type\n",
    "                            else:\n",
    "                                label = 'I-'+entity_type\n",
    "                        \n",
    "                        output_str = token[token_idx] + ' ' + label + '\\n'\n",
    "                        outputfile.write(output_str)\n",
    "                    start_tmp = end_pos\n",
    "\n",
    "        token = list(trainingset[article_id][start_tmp:])\n",
    "        whole_token = trainingset[article_id][start_tmp:]\n",
    "        for token_idx in range(len(token)):\n",
    "            if len(token[token_idx].replace(' ','')) == 0:\n",
    "                continue\n",
    "\n",
    "            \n",
    "            output_str = token[token_idx] + ' ' + 'O' + '\\n'\n",
    "            outputfile.write(output_str)\n",
    "\n",
    "        count = 0\n",
    "    \n",
    "        output_str = '\\n'\n",
    "        outputfile.write(output_str)\n",
    "        ID = trainingset[article_id]\n",
    "\n",
    "        if article_id%10 == 0:\n",
    "            print('Total complete articles:', article_id)\n",
    "\n",
    "    # close output file\n",
    "    outputfile.close()\n",
    "\n",
    "data_path='data/sample.data'\n",
    "CRFFormatData(train_set, position, data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load word2vec\n",
    "- pretrained traditional chinese word embedding by word2vec-cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained word vectors\n",
    "# get a dict of tokens (key) and their pretrained word vectors (value)\n",
    "# pretrained word2vec CBOW word vector: \n",
    "# https://fgc.stpi.narl.org.tw/activity/videoDetail/4b1141305ddf5522015de5479f4701b1\n",
    "dim = 0\n",
    "word_vecs= {}\n",
    "# open pretrained word vector file\n",
    "with open('cna.cbow.cwe_p.tar_g.512d.0.txt') as f:\n",
    "    for line in f:\n",
    "        tokens = line.strip().split()\n",
    "\n",
    "        # first line: vocabulary_size, word_vector_dim\n",
    "        if len(tokens) == 2:\n",
    "            dim = int(tokens[1])\n",
    "            continue\n",
    "    \n",
    "        word = tokens[0] \n",
    "        vec = np.array([ float(t) for t in tokens[1:] ])\n",
    "        word_vecs[word] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('vocabulary_size: ',len(word_vecs),\\\n",
    "      ' word_vector_dim: ',vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into train data & test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load `train.data` and separate into a list of labeled data of each text\n",
    "# return:\n",
    "#   data_list: a list of lists of tuples, storing tokens and labels (wrapped in tuple) of each text in `train.data`\n",
    "#   traindata_list: a list of lists, storing training data_list splitted from data_list\n",
    "#   testdata_list: a list of lists, storing testing data_list splitted from data_list\n",
    "from sklearn.model_selection import train_test_split\n",
    "def Dataset(data_path):\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        data=f.readlines()#.encode('utf-8').decode('utf-8-sig')\n",
    "    data_list, data_list_tmp = list(), list()\n",
    "    article_id_list=list()\n",
    "    idx=0\n",
    "    for row in data:\n",
    "        data_tuple = tuple()\n",
    "        if row == '\\n':\n",
    "            article_id_list.append(idx)\n",
    "            idx+=1\n",
    "            data_list.append(data_list_tmp)\n",
    "            data_list_tmp = []\n",
    "        else:\n",
    "            row = row.strip('\\n').split(' ')\n",
    "            data_tuple = (row[0], row[1])\n",
    "            data_list_tmp.append(data_tuple)\n",
    "    if len(data_list_tmp) != 0:\n",
    "        data_list.append(data_list_tmp)\n",
    "    \n",
    "    # here we random split data into training dataset and testing dataset\n",
    "    # but you should take `development data` or `test data` as testing data\n",
    "    # At that time, you could just delete this line, \n",
    "    # and generate data_list of `train data` and data_list of `development/test data` by this function\n",
    "    traindata_list, testdata_list, traindata_article_id_list, testdata_article_id_list=train_test_split(data_list,\n",
    "                                                                                                    article_id_list,\n",
    "                                                                                                    test_size=0.33,\n",
    "                                                                                                    random_state=42)\n",
    "    \n",
    "    return data_list, traindata_list, testdata_list, traindata_article_id_list, testdata_article_id_list \n",
    "\n",
    "data_list, traindata_list, testdata_list, traindata_article_id_list, testdata_article_id_list = Dataset(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look up word vectors\n",
    "# turn each word into its pretrained word vector\n",
    "# return a list of word vectors corresponding to each token in train.data\n",
    "def Word2Vector(data_list, embedding_dict):\n",
    "    embedding_list = list()\n",
    "\n",
    "    # No Match Word (unknown word) Vector in Embedding\n",
    "    unk_vector=np.random.rand(*(list(embedding_dict.values())[0].shape))\n",
    "\n",
    "    for idx_list in range(len(data_list)):\n",
    "        embedding_list_tmp = list()\n",
    "        for idx_tuple in range(len(data_list[idx_list])):\n",
    "            key = data_list[idx_list][idx_tuple][0] # token\n",
    "\n",
    "            if key in embedding_dict:\n",
    "                value = embedding_dict[key]\n",
    "            else:\n",
    "                value = unk_vector\n",
    "            embedding_list_tmp.append(value)\n",
    "        embedding_list.append(embedding_list_tmp)\n",
    "    return embedding_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input features: pretrained word vectors of each token\n",
    "# return a list of feature dicts, each feature dict corresponding to each token\n",
    "def Feature(embed_list):\n",
    "    feature_list = list()\n",
    "    for idx_list in range(len(embed_list)):\n",
    "        feature_list_tmp = list()\n",
    "        for idx_tuple in range(len(embed_list[idx_list])):\n",
    "            feature_dict = dict()\n",
    "            for idx_vec in range(len(embed_list[idx_list][idx_tuple])):\n",
    "                feature_dict['dim_' + str(idx_vec+1)] = embed_list[idx_list][idx_tuple][idx_vec]\n",
    "            feature_list_tmp.append(feature_dict)\n",
    "        feature_list.append(feature_list_tmp)\n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the labels of each tokens in train.data\n",
    "# return a list of lists of labels\n",
    "def Preprocess(data_list):\n",
    "    label_list = list()\n",
    "    for idx_list in range(len(data_list)):\n",
    "        label_list_tmp = list()\n",
    "        for idx_tuple in range(len(data_list[idx_list])):\n",
    "            label_list_tmp.append(data_list[idx_list][idx_tuple][1])\n",
    "        label_list.append(label_list_tmp)\n",
    "    return label_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Word Embedding\n",
    "trainembed_list = Word2Vector(traindata_list, word_vecs)\n",
    "testembed_list = Word2Vector(testdata_list, word_vecs)\n",
    "\n",
    "# CRF - Train Data (Augmentation Data)\n",
    "x_train = Feature(trainembed_list)\n",
    "y_train = Preprocess(traindata_list)\n",
    "\n",
    "# CRF - Test Data (Golden Standard)\n",
    "x_test = Feature(testembed_list)\n",
    "y_test = Preprocess(testdata_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python-crfsuite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import pycrfsuite\n",
    "trainer = pycrfsuite.Trainer(verbose=False)\n",
    "\n",
    "for xseq, yseq in zip(x_train, y_train):\n",
    "    trainer.append(xseq, yseq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.set_params({\n",
    "    'c1': 0.1,   # coefficient for L1 penalty\n",
    "    'c2': 0.1,  # coefficient for L2 penalty\n",
    "    'max_iterations': 100,  # stop earlier\n",
    "\n",
    "    # include transitions that are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trainer.train('model.crfsuite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_name = 'data/development_1.txt'\n",
    "def load_file(input_name):\n",
    "    with open(input_name, 'r') as dt:\n",
    "        file_text=dt.read().encode('utf-8').decode('utf-8-sig')\n",
    "    datas=file_text.split('\\n\\n--------------------\\n\\n')[:-1]\n",
    "    contents = []\n",
    "    for data in datas:\n",
    "        data = data.split('\\n')\n",
    "        contents.append(data[1])\n",
    "    return contents\n",
    "        \n",
    "contents = load_file(input_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_content(contents):\n",
    "    dta = []\n",
    "    for article_id in range(len(contents)):\n",
    "        set_split = list(contents[article_id])\n",
    "        while '' or ' ' in set_split:\n",
    "            if '' in set_split:\n",
    "                set_split.remove('')\n",
    "            else:\n",
    "                set_split.remove(' ')\n",
    "        dta.append(set_split)\n",
    "    return dta\n",
    "    \n",
    "dta = format_content(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list = Word2Vector(dta, word_vecs)\n",
    "pred = Feature(pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = pycrfsuite.Tagger()\n",
    "tagger.open('model.crfsuite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = [tagger.tag(xseq) for xseq in pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "def bio_classification_report(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Classification report for a list of BIO-encoded sequences.\n",
    "    It computes token-level metrics and discards \"O\" labels.\n",
    "    \n",
    "    Note that it requires scikit-learn 0.15+ (or a version from github master)\n",
    "    to calculate averages properly!\n",
    "    \"\"\"\n",
    "    lb = LabelBinarizer()\n",
    "    y_true_combined = lb.fit_transform(list(chain.from_iterable(y_true)))\n",
    "    y_pred_combined = lb.transform(list(chain.from_iterable(y_pred)))\n",
    "        \n",
    "    tagset = set(lb.classes_) - {'O'}\n",
    "    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n",
    "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
    "    \n",
    "    return classification_report(\n",
    "        y_true_combined,\n",
    "        y_pred_combined,\n",
    "        labels = [class_indices[cls] for cls in tagset],\n",
    "        target_names = tagset,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eva_pred = [tagger.tag(xseq) for xseq in x_test]\n",
    "print(bio_classification_report(y_test, eva_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output data\n",
    "* Change model output into `output.tsv` \n",
    "* Only accept this output format uploading to competition system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=\"article_id\\tstart_position\\tend_position\\tentity_text\\tentity_type\\n\"\n",
    "for test_id in range(len(y_pred)):\n",
    "    pos=0\n",
    "    start_pos=None\n",
    "    end_pos=None\n",
    "    entity_text=None\n",
    "    entity_type=None\n",
    "    for pred_id in range(len(y_pred[test_id])):\n",
    "        if y_pred[test_id][pred_id][0]=='B':\n",
    "            start_pos=pos\n",
    "            entity_type=y_pred[test_id][pred_id][2:]\n",
    "        elif start_pos is not None and y_pred[test_id][pred_id][0]=='I' and \\\n",
    "                (len(y_pred[test_id])-1==pred_id or y_pred[test_id][pred_id+1][0]=='O'):\n",
    "            end_pos=pos\n",
    "            entity_text=''.join([dta[test_id][position] for position in range(start_pos,end_pos+1)])\n",
    "            line=str(test_id)+'\\t'+str(start_pos)+'\\t'+str(end_pos+1)+'\\t'+entity_text+'\\t'+entity_type\n",
    "            output+=line+'\\n'\n",
    "        pos+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path='output.tsv'\n",
    "with open(output_path,'w',encoding='utf-8') as f:\n",
    "    f.write(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dm",
   "language": "python",
   "name": "dm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
